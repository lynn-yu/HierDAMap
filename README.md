<p align="center">

  <h1 align="center">HierDAMap: Towards Universal Domain Adaptive BEV Mapping via Hierarchical Perspective Priors</h1>
  <p align="center">
    <a href="https://www.researchgate.net/profile/Siyu-Li-45"><strong>Siyu Li</strong></a>
    ¬∑
    <a href="https://scholar.google.com/citations?user=j3364z8AAAAJ"><strong>Yihong Cao</strong></a>
    ¬∑
    <a href="https://scholar.google.com/citations?user=0EI9msQAAAAJ"><strong>Hao Shi</strong></a>
    ¬∑
    <a href=""><strong>Yongsheng Zang</strong></a>
    ¬∑
    <a href="https://github.com/mikasa3lili"><strong>Xuan He</strong></a>
    ¬∑
    <a href="https://yangkailun.com/"><strong>Kailun Yang‚Ä†</strong></a>
    ¬∑
    <a href="http://robotics.hnu.edu.cn/info/1071/1515.htm"><strong>Zhiyong Li‚Ä†</strong></a>
</p>

<p align="center">
    <a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
    <br>
    <a href="https://arxiv.org/pdf/2503.06821">
      <img src='https://img.shields.io/badge/Paper-green?style=for-the-badge&logo=adobeacrobatreader&logoWidth=20&logoColor=white&labelColor=66cc00&color=94DD15' alt='Paper PDF'>
    </a>
</p>

## Motivation
<div align=center>
<img src="https://github.com/lynn-yu/HierDAMap/blob/main/intro.png" >
</div>

## Framework
<div align=center>
<img src="https://github.com/lynn-yu/HierDAMap/blob/main/domainframe.png" >
</div>

### Abstract
The exploration of Bird's-Eye View (BEV) mapping technology has driven significant innovation in visual perception technology for autonomous driving.  BEV mapping models need to be applied to the unlabeled real world, making the study of unsupervised domain adaptation models an essential path. However, research on unsupervised domain adaptation for BEV mapping remains limited and cannot perfectly accommodate all BEV mapping tasks. To address this gap, this paper proposes HierDAMap, a universal and holistic BEV domain adaptation framework with hierarchical perspective priors. Unlike existing research that solely focuses on image-level learning using prior knowledge, this paper explores the guiding role of perspective prior knowledge across three distinct levels: global, sparse, and instance levels. With these priors, HierDA consists of three essential components, including Semantic-Guided Pseudo Supervision (SGPS), Dynamic-Aware Coherence Learning (DACL), and Cross-Domain Frustum Mixing (CDFM). SGPS constrains the cross-domain consistency of perspective feature distribution through pseudo labels generated by vision foundation models in 2D space. To mitigate feature distribution discrepancies caused by spatial variations, DACL employs uncertainty-aware predicted depth as an intermediary to derive dynamic BEV labels from perspective pseudo-labels, thereby constraining the coarse BEV features derived from corresponding perspective features. CDFM, on the other hand, leverages perspective masks of view frustum to mix multi-view perspective images from both domains, which guides cross-domain view transformation and encoding learning through mixed BEV labels. Furthermore, this paper introduces intra-domain feature exchange data augmentation to enhance the efficiency of domain adaptation learning.  The proposed method is verified on multiple BEV mapping tasks, such as BEV semantic segmentation, high-definition semantic, and vectorized mapping.  It demonstrates competitive performance across various conditions, including weather scenarios, regions, and datasets.

### Update
2025.4 Update HD mapping code
2025.3 Init repository.

### Acknowledgement
The code framework of this project is based on ![DualCross](https://github.com/YunzeMan/DualCross) and ![Semi-BEVseg](https://github.com/Junyu-Z/Semi-BEVseg), thanks to this excellent work.

## ü§ù Publication:
Please consider referencing this paper if you use the ```code``` from our work.
Thanks a lot :)

```
@article{li2025hierdamap,
  title={HierDAMap: Towards Universal Domain Adaptive BEV Mapping via Hierarchical Perspective Priors},
  author={Li, Siyu and Cao, Yihong and Shi, Hao and Zang, Yongsheng and He, Xuan and Yang, Kailun and Li, Zhiyong},
  journal={arXiv preprint arXiv:2503.06821},
  year={2025}
}
```
